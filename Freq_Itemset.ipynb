{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''Found this pretty good implementation of apriori'''\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "divider = 5                     # Divide every m/z by this number.\n",
    "max_len = int(10000 / divider)  # Max allowed m/z value.\n",
    "filter_peaks = 100              # How many peaks to keep after filteration.\n",
    "max_itemset_size = 7            # Max size of the frequent itemsets.\n",
    "min_support = 0.05              # Maybe we should increase this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To keep updating this dict for multiple files. Dont define it again.'''\n",
    "pep_spec = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic reader  \n",
    "You just need to provide the file path and it'll do everything for you. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "f=open(\"human_consensus_final_true_lib.msp\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "isName = isMW = isNumPeaks = False\n",
    "new = prev = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "while i < len(lines):\n",
    "    '''\n",
    "    Process each line of the file.\n",
    "    '''\n",
    "    line = lines[i]\n",
    "    i += 1\n",
    "    \n",
    "    splits = line.split(':')\n",
    "    \n",
    "    '''Keep going through lines. Name line is peptide and\n",
    "    all the lines after Num peaks are the spectrum'''\n",
    "    if splits[0] == 'Name':\n",
    "        split1 = splits[1]\n",
    "        pep = split1.split('/')[0].lstrip(' ')\n",
    "        isName = True\n",
    "        \n",
    "    if isName and splits[0] == 'MW':\n",
    "        mass = float(splits[1]) # Not using it anywhere. It's just there.\n",
    "        isMW = True\n",
    "        \n",
    "    if isName and isMW and splits[0] == 'Num peaks':\n",
    "        '''After this line, we will start processing the spectrum.'''\n",
    "        numPeaks = int(splits[1]) # Just in case.\n",
    "        \n",
    "        temp_spec = np.zeros(max_len)\n",
    "        while (lines[i] != '\\n'):\n",
    "            mzline = lines[i]\n",
    "            i += 1\n",
    "            mzsplits = mzline.split('\\t')\n",
    "            # Get the m/z and intenisty value of each line.\n",
    "            moz, intensity = float(mzsplits[0]), float(mzsplits[1])\n",
    "            # Dividing m/z by the divider.\n",
    "            temp_spec[round(moz / divider)] = intensity\n",
    "        \n",
    "        isNumPeaks = True\n",
    "        \n",
    "    if isName and isMW and isNumPeaks:\n",
    "        '''At this point, we are done reading one spectrum.\n",
    "        Place everything where it needs to go and read the next one.'''\n",
    "        isName = isMW = isNumPeaks = False\n",
    "        \n",
    "        # This is the filter. Gets the top 100 peaks\n",
    "        # and returns them in one hot encoded vector of size max_len\n",
    "        top_indx = np.argpartition(temp_spec, -filter_peaks)[-filter_peaks:]\n",
    "        spec = np.zeros(max_len)\n",
    "        spec[top_indx] = 1\n",
    "        \n",
    "        # Creating dictionary. Key: peptide, Value: spectrum.\n",
    "        # As a side effect we'll get rid of duplicates.\n",
    "        if pep not in pep_spec: pep_spec[pep] = spec\n",
    "        \n",
    "        # Displays the progress.\n",
    "        new = int((i/len(lines)) * 100)\n",
    "        if new > prev:\n",
    "            clear_output(wait=True)\n",
    "            print(str(new) + '%')\n",
    "            prev = new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207910\n"
     ]
    }
   ],
   "source": [
    "# Just get the value from the dict.\n",
    "buckets = list(pep_spec.values())\n",
    "print(len(buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The apriori function needs a dataframe.\n",
    "df_buckets = pd.DataFrame(buckets[0:50000], columns=list(range(0, max_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation of apriori  \n",
    "http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_itemsets = apriori(df_buckets, \n",
    "                        min_support=min_support, \n",
    "                        max_len=max_itemset_size, \n",
    "                        verbose=1, \n",
    "                        low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
